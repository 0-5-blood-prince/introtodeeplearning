{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab2_Part2-Solutions.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aamini/introtodeeplearning_labs/blob/2019/lab2/Lab2_Part2_Solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Ag_e7xtTzT1W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Laboratory 2 : Computer Vision\n",
        "\n",
        "# Part 2: De-Biasing Facial Recognition Systems\n",
        "\n",
        "In the second portion of this lab, we will explore two prominent aspects of applied deep learning: facial recognition systems and algorithmic bias. \n",
        "\n",
        "Deploying fair, unbiased AI systems is critical for long-term acceptance of these approaches. Consider the task of facial recognition: given an image, is it an image of a face? [Recent work from the MIT Media Lab](http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf) showed that this seemingly simple, but extremely important, task is subject to extreme amounts of algorithmic bias among select demographics. [Another report](https://ieeexplore.ieee.org/document/6327355) analyzed the face detection system used by the US law enforcement, and found that it had significantly lower accuracy among dark women between the age of 18-30 years old. Run the next code block for a short video from Joy B. that explains this notion of algorithmic bias:"
      ]
    },
    {
      "metadata": {
        "id": "XQh5HZfbupFF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "33a937a5-bc6c-4fc4-c016-25d248654fc2"
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo('162VzSzzoPs')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"400\"\n",
              "            height=\"300\"\n",
              "            src=\"https://www.youtube.com/embed/162VzSzzoPs\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkz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\nAD//2Q==\n",
            "text/plain": [
              "<IPython.lib.display.YouTubeVideo at 0x7efecebe9490>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "3Ezfc6Yv6IhI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this lab, we'll investigate one approach to address algorithmic bias by building a facial recognition model that learns the underlying *latent variables* in a dataset and uses this to adaptively re-sample the training data, mitigating any bias. This lab is based on a very recent paper in which this approach was originially proposed.   \n",
        "\n",
        "Let's get started by installing the relevant dependencies:"
      ]
    },
    {
      "metadata": {
        "id": "E46sWVKK6LP9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO: install dependencies"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V0e77oOM3udR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.1 Datasets\n",
        "\n",
        "We'll be using three datasets in this lab. In order to train our facial recognition models, we'll need a dataset of positive examples (i.e., of faces) and a dataset of negative examples (i.e., of things that are not faces). Finally, we'll need a test dataset of face images. Since we're concerned about the potential *bias* of our learned models against certain demographics, it's important that the test dataset we use has equal representation across the demographics or features of interest. We'll specifically be looking at skin tone and gender. \n",
        "\n",
        "\n",
        "1.   Positive training data: [CelebA Dataset](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html). A large-scale (over 200K images) of celebrity faces.   \n",
        "2.   Negative training data: [ImageNet](http://www.image-net.org/). Many images across many different categories. We'll take negative examples from a variety of non-human categories. \n",
        "3. Test data: [Pilot Parliaments Benchmark](http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf) (PPB). Images of parliamentarians from three African countries and three European countries, selected for parity across gender and skin tone. \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "CQ146crd6RdA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's begin by importing these three datasets. We've written a function that does a bit of data pre-processing and imports these data for you :)"
      ]
    },
    {
      "metadata": {
        "id": "RWXaaIWy6jVw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO function to import the datasets in the appropriate format"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sxtkJoqF6oH1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To get a better sense of what's in each of these datasets, we can display some randomly selected images from each."
      ]
    },
    {
      "metadata": {
        "id": "4B4egQZY6wEt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO display images from the three datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JpgJdNyJ60dy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As you can see, the PPB dataset is quite balanced in terms of the skin tone and gender across the set of displayed images. Do you notice any trends or patterns in the images from the CelebA dataset? Do you anticipate any potential issues in terms of classification performance for models trained on CelebA and then tested on a dataset like PPB?"
      ]
    },
    {
      "metadata": {
        "id": "NDj7KBaW8Asz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Thinking about bias\n",
        "\n",
        "Remember that we'll be training facial detection classifiers on the large, well-curated CelebA dataset (and ImageNet), and then evaluate the *accuracy* and *bias* of our models across different demographics by testing them on the PPB dataset. Our goal is to build a model that trains on CelebA *and* achieves high classification accuracy on PPB across all demographics, and to show that this model does not suffer from algorithmic bias. \n",
        "\n",
        "What exactly do we mean if we say a classifier is biased? In order to formalize this, we'll need to think about [*latent variables*](https://en.wikipedia.org/wiki/Latent_variable), variables that define a dataset but are not strictly observed, which was introduced during the generative modeling lecture. We can think of a classifier as *biased* if its classification decision changes after it sees some additional latent features. This notion of bias will be helpful to keep in mind throughout the rest of the lab. "
      ]
    },
    {
      "metadata": {
        "id": "AIFDvU4w8OIH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.2 CNN for facial detection \n",
        "\n",
        "First, we'll define and train a CNN on the facial classification task, and evaluate its accuracy on the PPB dataset. Later, we'll evaluate the performance of our debiased models against this baseline CNN. The model architecture is shown here:\n",
        "\n",
        "![CNN model](https://raw.githubusercontent.com/aamini/introtodeeplearning_labs/2019/lab2/img/mnist_model.png)\n",
        "TODO: update the architecture figure"
      ]
    },
    {
      "metadata": {
        "id": "1U32Y7HsBj6V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define and train the CNN model\n",
        "\n",
        "Like we did in the first part of the lab, we'll define our CNN model, and then train on the CelebA and ImageNet datasets using the `tf.GradientTape` class and the `tf.GradientTape.gradient` method:"
      ]
    },
    {
      "metadata": {
        "id": "82EVTAAW7B_X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO: block here for defining the CNN model and running the training loop"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LZdsAzFKB5jW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can examine the classification accuracy of our trained CNN on an independent validation set drawn from CelebA and also look at how the loss evolved as training progressed:\n",
        "\n",
        "TODO: have an image of loss and validation accuracy here?"
      ]
    },
    {
      "metadata": {
        "id": "AKMdWVHeCxj8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Evaluate performance on the PPB dataset\n",
        "\n",
        "Next, let's evaluate the classification performance of our CelebA-trained CNN on the PPB dataset. We'll look at the classification accuracy across four different demographics defined in PPB: dark-skinned male, dark-skinned female, light-skinned male, and light-skinned female."
      ]
    },
    {
      "metadata": {
        "id": "HL38a0EdCiux",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO: block here for testing the CNN on PPB\n",
        "# include a function that returns / plots the accuracy on the four different demographics in PPB"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j0Cvvt90DoAm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Take a look at the accuracies for this first model across these four groups. What do you observe? Would you consider this model biased or unbiased, and why? "
      ]
    },
    {
      "metadata": {
        "id": "nLemS7dqECsI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.3 Variational autoencoder (VAE) for learning latent structure\n",
        "\n",
        "As you saw, the accuracy of the CNN varies across the four demographics we looked at, performing worse on images of dark-skinned males in particular. To think about why  let's consider the dataset the model was trained on, CelebA. If certain features, such as dark skin or hats, are *rare* in CelebA, the model may end up biased against these; that is, it's classification accuracy will be worse on dark-skinned faces or faces with hats. This is a problem. \n",
        "\n",
        "TODO: insert schematic image here?"
      ]
    },
    {
      "metadata": {
        "id": "cKkzPoOmKtf7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To debias our classifier, we'll train a model that learns the latent features underlying the faces in the training set, and then uses this information to mitigate unwanted biases by sampling faces with rare features, like dark skin or hats, *more frequently *while it trains. So, our model needs to learn an *encoding* of the latent features in an entirely unsupervised way -- we'll turn to a variational autoencoder (VAE):\n",
        "\n",
        "![The concept of a VAE](http://kvfrans.com/content/images/2016/08/vae.jpg)\n",
        "\n",
        "TODO: change the VAE architecture\n",
        "\n",
        "VAEs rely on this encoder-decoder structure. The encoder takes in the input images, encodes them into a series of variables defined by a mean and standard deviation, and then samples from these to generate a set of sampled latent variables. The decoder then \"decodes\" these variables to generate a reconstruction of the original image, which is used during training to help the model learn which latent variables are important. "
      ]
    },
    {
      "metadata": {
        "id": "vPLAHY7XK5UA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define and train the VAE model\n",
        "\n",
        "We'll build our VAE model by defining the encoder, which will consist of a series of convolutional layers like our standard CNN, and the decoder, which will consist of a series of deconvolutional layers that will lead to a reconstruction of the input image. "
      ]
    },
    {
      "metadata": {
        "id": "zk6UZ1PNK8SA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO code block for defining the VAE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e8c9cPxOLihM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As with our CNN model, we'll train our VAE on the facial classification task using the CelebA dataset as positive examples and images drawn from ImageNet as negative examples.\n",
        "\n",
        "Complete and execute the training loop to train the VAE!"
      ]
    },
    {
      "metadata": {
        "id": "WhzLjz02L8cX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO code block for training the VAE on celebA/ImageNet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9iUUMzhHMAN4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### What latent variables did the VAE learn?\n",
        "\n",
        "To get a better sense of what the VAE is learning, we can perturb the value of a single learned variable individually, and output the reconstructed faces resulting from each of these perturbations. This gives us the reconstructed output of the VAE along the gradient of a single latent variable. \n",
        "\n",
        "Run the code block below to examine some of the learned latent features!"
      ]
    },
    {
      "metadata": {
        "id": "sBVC0knxMv3v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "# Either have a block for face reconstruction slider\n",
        "# Or can just have images (already outputted) for perturbations for a few latent variables"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LVCZvMURNCa1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Hopefully you were able to observe that the VAE is able to learn qualitative features in the face dataset such as skin tone, hair, and gaze direction (azimuth). This examination gives us a sense of the types of features that our model will learn to debias against."
      ]
    },
    {
      "metadata": {
        "id": "qtHEYI9KNn0A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.4 Debiasing variational autoencoder (DB-VAE) for facial detection\n",
        "\n",
        "### The DB-VAE architecture\n",
        "\n",
        "Now, we're ready to use the latent variables learned by a VAE to adaptively re-sample the CelebA data during the training operation. Specifically, we will alter the probability that a given image is sampled during training based on how often its features appear in the dataset. So, faces with rarer features (like dark skin, sunglasses, or hats) should be more likely to be sampled during training. The resulting model is termed a *debiasing variational autoencoder*, or DB-VAE:\n",
        "\n",
        "![DB-VAE](https://raw.githubusercontent.com/aamini/introtodeeplearning_labs/2019/lab2/img/DB-VAE.png)\n",
        "\n",
        "TODO: caption for the image"
      ]
    },
    {
      "metadata": {
        "id": "ziA75SN-UxxO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The network architecture of the DB-VAE is very similar to the VAE model we worked with earlier, and the encoder uses the same convolutional architecture as our CNN from earlier!\n",
        "\n",
        "Note how the encoder portion learns a single supervised variable, *z0*, corresponding to the classification prediction--face or not face. Keep in mind that we only want to learn the latent representation of *faces*, as that's what we're ultimately debiasing against. So, our DB-VAE model will only learn unsupervised latent variables for the images of faces in addition to outputting the supervised classification prediction *z0*, but will instead only output *z0* for the negative examples. \n"
      ]
    },
    {
      "metadata": {
        "id": "YIu_2LzNWwWY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Now we're ready to define the DB-VAE architecture:"
      ]
    },
    {
      "metadata": {
        "id": "PVzs2hefVEVo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO code block to define/build DB-VAE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nbDNlslgQc5A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Adaptive resampling for automated debiasing with DB-VAE\n",
        "\n",
        "Let's think a bit more about the math behind the described resampling operation. As shown above, as the input images are fed through the network, the encoder learns an estimate *Q(z|X)* of the latent distribution. We want to increase the relative frequency of rare data by increased sampling of under-represented regions of the latent space. So, we'll approximate *Q(z|X)* using the frequency distributions of each of the learned latent variables, and then use this approximation to define the probability distribution of selecting a given datapoint *x*. These probability distributions will be used during training to re-sample the data.\n",
        "\n",
        "TODO: define math here? Possible to have equation text within text block of notebook?\n"
      ]
    },
    {
      "metadata": {
        "id": "jqaasVtIUQ0G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Complete the code block below to define the resampling algorithm within the DB-VAE training loop. "
      ]
    },
    {
      "metadata": {
        "id": "P0HrlxPSUgle",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO code block for the DB-VAE training operation with the resampling/de-biasing algorithm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pF14fQkVUs-a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Great! Now let's train our debiased model using our CelebA/ImageNet training data. Note that we only want to debias for features relevant to *faces*, not the set of negative examples. So, we will not learn a set of unsupervised latent variables for the negative examples, and instead output a single supervised variable reflecting the classification prediction (face vs. not face)."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "9YR8U43FVZ_8",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO function call for the DB-VAE training operation."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uZBlWDPOVcHg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As a check, we can examine the loss and classification accuracy for our debiased DB-VAE model on an independent validation set drawn from CelebA, and compare these to our biased CNN model:"
      ]
    },
    {
      "metadata": {
        "id": "yUwJB9K8Xn-O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO code block for evaluating on validation set and plotting? \n",
        "# not sure if this is necessary????"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tEsh7DIXXt5m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![loss](https://raw.githubusercontent.com/aamini/introtodeeplearning_labs/2019/lab2/img/loss_val-acc.png)\n",
        "\n",
        "TODO: example image, decide if actually want to show a plot like this"
      ]
    },
    {
      "metadata": {
        "id": "Eo34xC7MbaiQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.4 Evaluation on Pilot Parliaments Benchmark (PPB) Dataset\n",
        "\n",
        "Now we're finally ready to test our debiased DB-VAE model on the[ PPB dataset](http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf). The PPB dataset consists of images of 1270 male and female parliamentarians from various African and European countries and exhibits parity in both skin tone and gender. The gender of each face is annotated with the sex-based \"Male'' and \"Female'' labels. Skin tone annotations are based on the Fitzpatrick skin type classification system, with each image labeled as \"Lighter'' or \"Darker''.\n",
        "\n",
        "We'll evaluate both the overall accuracy of the DB-VAE as well as its accuracy on each the \"Dark Male\", \"Dark Female\", \"Light Male\", and \"Light Female\" demographics, and compare the performance of this debiased model against the biased CNN from earlier in the lab. \n",
        "\n",
        "Here are some example faces and average faces from the PPB dataset:\n",
        "![PPB Example Images](https://raw.githubusercontent.com/aamini/introtodeeplearning_labs/2019/lab2/img/PPB%20faces.png)"
      ]
    },
    {
      "metadata": {
        "id": "ruzxwzo2ko6N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To assess the performance of our models, we'll consider the classification accuracy, which we define as the fraction of faces detected in the PPB dataset by a given model. By comparing the accuracy of a model without debiasing and our DB-VAE model, we can get a sense of how effectively we were able to debias against features like skin tone and gender.\n"
      ]
    },
    {
      "metadata": {
        "id": "p00yZqAFoFJy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's evaluate each of the standard CNN and our debiased DB-VAE on the PPB test dataset:"
      ]
    },
    {
      "metadata": {
        "id": "bgK77aB9oDtX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO code block to run the evaluation of the CNN and the DB-VAE on PPB dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F-3NzMB0oQtv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can calculate the accuracies of our two models on the whole PPB dataset as well as across the four demographics proposed:"
      ]
    },
    {
      "metadata": {
        "id": "zzm-THVJkBjY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO code block to calculate the accuracies, and to plot the result in bar graph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HK3KwNIWoiXi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can visualize these results to enable a direct comparison: \n",
        "\n",
        "TODO generate bar graph and insert, should look like something else, but difference should be clear. this is something the students could do on their own as well. "
      ]
    },
    {
      "metadata": {
        "id": "wOwvbNX3oswJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO plot the accuracy bar graph for the two models on PPB, across the 4 demographics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "srrEPgJko5a7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO generate a table showing the accuracies for the 4 demographics for each model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rESoXRPQo_mq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.5 Conclusion \n",
        "\n",
        "We encourage you to think about and maybe even address some questions raised by the approach and results outlined here:\n",
        "\n",
        "*  How does the accuracy of the DB-VAE across the four demographics compare to that of the standard CNN? Do you find this result surprising in any way?\n",
        "*  In which applications (either related to facial detection or not!) would debiasing in this way be desired? Are there applications where you may not want to debias your model? \n",
        "* Do you think it should be necessary for companies to demonstrate that their models, particularly in the context of tasks like facial detection, are not biased? If so, do you have thoughts on how this could be standardized and implemented?\n",
        "* Do you have ideas for other ways to address issues of bias, particularly in terms of the training data?\n",
        "\n",
        "Hopefully this lab has shed some light on a few concepts, from vision based tasks, to VAEs, to algorithmic bias. We like to think it has, but we're biased ;). \n",
        "\n",
        "![Faces](https://media1.tenor.com/images/44e1f590924eca94fe86067a4cf44c72/tenor.gif?itemid=3394328)"
      ]
    },
    {
      "metadata": {
        "id": "K5D4HikrqS8X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}