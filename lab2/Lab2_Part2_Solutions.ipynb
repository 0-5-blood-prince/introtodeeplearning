{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab2_Part2-Solutions.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aamini/introtodeeplearning_labs/blob/2019/lab2/Lab2_Part2_Solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Ag_e7xtTzT1W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Laboratory 2 : Computer Vision\n",
        "\n",
        "# Part 2: De-Biasing Facial Recognition Systems\n",
        "\n",
        "In the second portion of this lab, we will explore two prominent aspects of applied deep learning: facial recognition systems and algorithmic bias. \n",
        "\n",
        "Deploying fair, unbiased AI systems is critical for long-term acceptance of these approaches. Consider the task of facial recognition: given an image, is it an image of a face? [Recent work from the MIT Media Lab](http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf) showed that this seemingly simple, but extremely important, task is subject to extreme amounts of algorithmic bias among select demographics. [Another report](https://ieeexplore.ieee.org/document/6327355) analyzed the face detection system used by the US law enforcement, and found that it had significantly lower accuracy among dark women between the age of 18-30 years old. These results are especially concerning since these facial recognition systems are almost never deployed in isolation.\n",
        "\n",
        "We'll investigate one approach to addressing this problem by building a facial recognition model that learns the underlying *latent variables* in a dataset and uses this to adaptively re-sample the training data, mitigating any bias. This lab is based on a very recent paper in which this approach was originially proposed.   \n",
        "\n",
        "Let's get started.\n",
        "\n",
        "TODO: cite Face paper"
      ]
    },
    {
      "metadata": {
        "id": "3Ezfc6Yv6IhI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First let's install the relevant dependencies:"
      ]
    },
    {
      "metadata": {
        "id": "E46sWVKK6LP9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO: install dependencies"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V0e77oOM3udR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.1 Datasets\n",
        "\n",
        "We'll be using three datasets in this lab. In order to train our facial recognition models, we'll need a dataset of positive examples (i.e., of faces) and a dataset of negative examples (i.e., of things that are not faces). Finally, we'll need a test dataset of face images. Since we're concerned about the potential *bias* of our learned models against certain demographics, it's important that the test dataset we use has equal representation across the demographics or features of interest. We'll specifically be looking at skin tone and gender. \n",
        "\n",
        "\n",
        "1.   Positive training data: [CelebA Dataset](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html). A large-scale (over 200K images) of celebrity faces.   \n",
        "2.   Negative training data: [ImageNet](http://www.image-net.org/). Many images across many different categories. We'll take negative examples from a variety of non-human categories. \n",
        "3. Test data: [Pilot Parliaments Benchmark](http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf) (PPB). Images of parliamentarians from three African countries and three European countries, selected for parity across gender and skin tone. \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "CQ146crd6RdA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's begin by importing these three datasets. We've written a function that does a bit of data pre-processing and imports these data for you :)"
      ]
    },
    {
      "metadata": {
        "id": "RWXaaIWy6jVw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO function to import the datasets in the appropriate format"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sxtkJoqF6oH1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To get a better sense of what's in each of these datasets, we can display some randomly selected images from each."
      ]
    },
    {
      "metadata": {
        "id": "4B4egQZY6wEt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO display images from the three datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JpgJdNyJ60dy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As you can see, the PPB dataset is quite balanced in terms of the skin tone and gender across the set of displayed images. Do you notice any trends or patterns in the images from the CelebA dataset? Do you anticipate any potential issues in terms of classification performance for models trained on CelebA and then tested on a dataset like PPB?"
      ]
    },
    {
      "metadata": {
        "id": "NDj7KBaW8Asz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Thinking about bias\n",
        "\n",
        "Remember that we'll be training facial detection classifiers on the large, well-curated CelebA dataset (and ImageNet), and then evaluate the *accuracy* and *bias* of our models across different demographics by testing them on the PPB dataset. Our goal is to build a model that trains on CelebA *and* achieves high classification accuracy on PPB across all demographics, and to show that this model does not suffer from algorithmic bias. \n",
        "\n",
        "What exactly do we mean if we say a classifier is biased? In order to formalize this, we'll need to think about [*latent variables*](https://en.wikipedia.org/wiki/Latent_variable), variables that define a dataset but are not strictly observed, which was introduced during the generative modeling lecture. We can think of a classifier as *biased* if its classification decision changes after it sees some additional latent features. This notion of bias will be helpful to keep in mind throughout the rest of the lab. "
      ]
    },
    {
      "metadata": {
        "id": "AIFDvU4w8OIH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.2 CNN for facial detection \n",
        "\n",
        "First, we'll define and train a CNN on the facial classification task, and evaluate its accuracy on the PPB dataset. Later, we'll evaluate the performance of our debiased models against this baseline CNN. The model architecture is shown here:\n",
        "\n",
        "![CNN model](https://raw.githubusercontent.com/aamini/introtodeeplearning_labs/2019/lab2/img/mnist_model.png)\n",
        "TODO: update the architecture figure"
      ]
    },
    {
      "metadata": {
        "id": "1U32Y7HsBj6V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define and train the CNN model\n",
        "\n",
        "Like we did in the first part of the lab, we'll define our CNN model, and then train on the CelebA and ImageNet datasets using the `tf.GradientTape` class and the `tf.GradientTape.gradient` method:"
      ]
    },
    {
      "metadata": {
        "id": "82EVTAAW7B_X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO: block here for defining the CNN model and running the training loop"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LZdsAzFKB5jW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can examine the classification accuracy of our trained CNN on an independent validation set drawn from CelebA and the loss evolution as training progressed:\n",
        "\n",
        "TODO: have an image of loss and validation accuracy here?"
      ]
    },
    {
      "metadata": {
        "id": "AKMdWVHeCxj8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Evaluate performance on the PPB dataset\n",
        "\n",
        "Next, let's evaluate the classification performance of our CelebA-trained CNN on the PPB dataset. We'll look at the classification accuracy across four different demographics defined in PPB: dark-skinned male, dark-skinned female, light-skinned male, and light-skinned female."
      ]
    },
    {
      "metadata": {
        "id": "HL38a0EdCiux",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO: block here for testing the CNN on PPB\n",
        "# include a function that returns / plots the accuracy on the four different demographics in PPB"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j0Cvvt90DoAm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Take a look at the accuracies for this first model across these four groups. What do you observe? Would you consider this model biased or unbiased, and why? "
      ]
    },
    {
      "metadata": {
        "id": "nLemS7dqECsI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.3 Variational autoencoder (VAE) for learning latent structure\n",
        "\n",
        "As you saw, the accuracy of the CNN varies across the four demographics we looked at, performing worse on images of dark-skinned males in particular. To think about why  let's consider the dataset the model was trained on, CelebA. If certain features, such as dark skin or hats, are *rare* in CelebA, the model may end up biased against these; that is, it's classification accuracy will be worse on dark-skinned faces or faces with hats. This is a problem. \n",
        "\n",
        "TODO: insert schematic image here?"
      ]
    },
    {
      "metadata": {
        "id": "cKkzPoOmKtf7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To debias our classifier, we'll train a model that learns the latent features underlying the faces in the training set, and then uses this information to mitigate unwanted biases by sampling faces with rare features, like dark skin or hats, *more frequently *while it trains. So, our model needs to learn an *encoding* of the latent features in an entirely unsupervised way -- we'll turn to a variational autoencoder (VAE):\n",
        "\n",
        "![The concept of a VAE](http://kvfrans.com/content/images/2016/08/vae.jpg)\n",
        "\n",
        "TODO: change the VAE architecture\n",
        "\n",
        "VAEs rely on this encoder-decoder structure. The encoder takes in the input images, encodes them into a series of variables defined by a mean and standard deviation, and then samples from these to generate a set of sampled latent variables. The decoder then \"decodes\" these variables to generate a reconstruction of the original image, which is used during training to help the model learn which latent variables are important. "
      ]
    },
    {
      "metadata": {
        "id": "vPLAHY7XK5UA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define and train the VAE model\n",
        "\n",
        "We'll build our VAE model by defining the encoder, which will consist of a series of convolutional layers like our standard CNN, and the decoder, which will consist of a series of deconvolutional layers that will lead to a reconstruction of the input image. "
      ]
    },
    {
      "metadata": {
        "id": "zk6UZ1PNK8SA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO code block for defining the VAE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e8c9cPxOLihM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As with our CNN model, we'll train our VAE on the facial classification task using the CelebA dataset as positive examples and images drawn from ImageNet as negative examples.\n",
        "\n",
        "Complete and execute the training loop to train the VAE!"
      ]
    },
    {
      "metadata": {
        "id": "WhzLjz02L8cX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO code block for training the VAE on celebA/ImageNet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9iUUMzhHMAN4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### What latent variables did the VAE learn?\n",
        "\n",
        "To get a better sense of what the VAE is learning, we can perturb the value of a single learned variable individually, and output the reconstructed faces following each of these perturbations. This gives us the reconstructed output of the VAE along the gradient of a single latent variable. \n",
        "\n",
        "Run the code block below to examine some of the learned latent features!"
      ]
    },
    {
      "metadata": {
        "id": "sBVC0knxMv3v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "# Either have a block for face reconstruction slider\n",
        "# Or can just have images (already outputted) for perturbations for a few latent variables"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LVCZvMURNCa1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Hopefully you were able to observe that the VAE is able to learn qualitative features in the face dataset such as skin tone, hair, and gaze direction (azimuth). This examination gives us a sense of the types of features that our model will learn to debias against."
      ]
    },
    {
      "metadata": {
        "id": "qtHEYI9KNn0A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.4 Debiasing variational autoencoder (DB-VAE) for facial detection\n",
        "\n"
      ]
    }
  ]
}