{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab1_Music.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "O-97SDET3JG-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lab 1 : Intro to TensorFlow and Music Generation with RNNs\n",
        "\n",
        "# Part 2: Music Generation with RNNs\n",
        "\n",
        "In this portion of the lab, we will explore building a Recurrent Neural Network (RNN) for music generation. We will train a model to learn the patterns in raw sheet music in [ABC notation](https://en.wikipedia.org/wiki/ABC_notation), and then use this model to generate new music. "
      ]
    },
    {
      "metadata": {
        "id": "oJ4aiwAq3j6X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.1 Dependencies \n",
        "First, let's download the course repository, install dependencies, and import the relevant packages we'll need for this lab."
      ]
    },
    {
      "metadata": {
        "id": "GW44BHHsw172",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget https://developer.nvidia.com/compute/cuda/10.0/Prod/local_installers/cuda-repo-ubuntu1604-10-0-local-10.0.130-410.48_1.0-1_amd64 -O cuda-repo-ubuntu1604-10-0-local-10.0.130-410.48_1.0-1_amd64.deb\n",
        "!dpkg -i cuda-repo-ubuntu1604-10-0-local-10.0.130-410.48_1.0-1_amd64.deb\n",
        "!apt-key add /var/cuda-repo-10-0-local-10.0.130-410.48/7fa2af80.pub\n",
        "!apt-get update\n",
        "!apt-get install cuda\n",
        "!pip install tf-nightly-gpu-2.0-preview"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "riVZCVK65QTH",
        "colab_type": "code",
        "outputId": "1bcdf8bc-4e15-47e7-8f8f-957c0852019b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/aamini/introtodeeplearning_labs.git\n",
        "% cd introtodeeplearning_labs\n",
        "! git checkout 2019\n",
        "% cd .."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'introtodeeplearning_labs'...\n",
            "remote: Enumerating objects: 66, done.\u001b[K\n",
            "remote: Counting objects: 100% (66/66), done.\u001b[K\n",
            "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
            "remote: Total 315 (delta 41), reused 13 (delta 7), pack-reused 249\u001b[K\n",
            "Receiving objects: 100% (315/315), 49.48 MiB | 30.49 MiB/s, done.\n",
            "Resolving deltas: 100% (121/121), done.\n",
            "/content/introtodeeplearning_labs\n",
            "Branch '2019' set up to track remote branch '2019' from 'origin'.\n",
            "Switched to a new branch '2019'\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lt-3uKCa5wU3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from introtodeeplearning_labs.lab1.util import util as util\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FwnMeYBa4wmA",
        "colab_type": "code",
        "outputId": "82c58029-c24e-4092-a595-09543bfd1e3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf \n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "is_correct_tf_version = '1.13.' in tf.__version__\n",
        "assert is_correct_tf_version, \"Wrong tensorflow version ({}) installed\".format(tf.__version__)\n",
        "\n",
        "is_eager_enabled = tf.executing_eagerly()\n",
        "assert is_eager_enabled,      \"Tensorflow eager mode is not enabled\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-e5b033813ff6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mis_correct_tf_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'1.13.'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mis_correct_tf_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Wrong tensorflow version ({}) installed\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mis_eager_enabled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Wrong tensorflow version (1.12.0) installed"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "cFChkRWVs40s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install --force https://github.com/chengs/tqdm/archive/colab.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SGRGjO3VtGF2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm_notebook as tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_ajvp0No4qDm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 2.2 Dataset\n",
        " We've gathered a dataset of thousands of [Irish folk songs](https://www.youtube.com/watch?v=2Z_TheGgFWI), represented in the ABC notation. Let's download the dataset: "
      ]
    },
    {
      "metadata": {
        "id": "P7dFnP5q3Jve",
        "colab_type": "code",
        "outputId": "c6cb6155-1b2f-4c88-e9dc-f5a6d7311451",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('irish.abc', 'https://raw.githubusercontent.com/aamini/introtodeeplearning_labs/2019/lab1/data/irish.abc')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/aamini/introtodeeplearning_labs/2019/lab1/data/irish.abc\n",
            "204800/197618 [===============================] - 0s 0us/step\n",
            "212992/197618 [================================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "UHjdCjDuSvX_"
      },
      "cell_type": "markdown",
      "source": [
        "### Inspect the dataset\n",
        "\n",
        "We can take a look to get a better sense of the dataset:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "aavnuByVymwK",
        "outputId": "5adab2a3-c68e-4d65-edbd-93bd62a9f3ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "text = open(path_to_file).read()\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 197618 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ww73Nh8xhzd0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can grabe a song from our dataset as an example and play it back: \n",
        "\n",
        "TODO: will have a function here that grabs a song and then plays it back as an example"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Duhg9NrUymwO",
        "outputId": "1362e390-e7c2-4ad6-f464-630ec303ecb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X:1\n",
            "T:Alexander's\n",
            "Z: id:dc-hornpipe-1\n",
            "M:C|\n",
            "L:1/8\n",
            "K:D Major\n",
            "(3ABc|dAFA DFAd|fdcd FAdf|gfge fefd|(3efe (3dcB A2 (3ABc|!\n",
            "dAFA DFAd|fdcd FAdf|gfge fefd|(3efe dc d2:|!\n",
            "AG|FAdA FAdA|GBdB GBdB|Acec Acec|dfaf gecA|!\n",
            "FAdA FAdA|GBdB GBdB|Aceg fefd|(3efe dc d2:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7vH24yyquwKQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "One important thing to think about is how many different characters are present in the text file. This will become important soon, when we generate a numerical representation for the text data:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "IlCgQBRVymwR",
        "outputId": "e8be7c03-2c35-4d9e-b870-ba984f4b723a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "83 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lwSjN0CMiJtX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<!-- TODO: here explanation of the one-hot encoding, getting the unique characters in the file -->"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "rNnrKn_lL-IJ"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.3 Process the dataset for the learning task"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "wssHQ1oGymwe"
      },
      "cell_type": "markdown",
      "source": [
        "Let's take a step back and consider our prediction task. We're trying to train a RNN model to learn patterns in ABC music, and then use this model to generate (i.e., predict) a new piece of music based on this learned information. \n",
        "\n",
        "Breaking this down, what we're really asking the model is: given a character, or a sequence of characters, what is the most probable next character? We'll train the model to perform this task. \n",
        "\n",
        "To achieve this, we will input a sequence of characters to the model, and train the model to predict the output, that is, the following character at each time step. RNNs maintain an internal state that depends on previously seen elements, so we information about all characters seen up until a given moment will be taken into account in generating the prediction.\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "LFjSVAlWzf-N"
      },
      "cell_type": "markdown",
      "source": [
        "### Vectorize the text\n",
        "\n",
        "Before we begin training our RNN model, we'll need to create a numerical representation of our text-based dataset. To do this, we'll generate two lookup tables: one that maps characters to numbers, and a second that maps numbers back to characters. Recall that we just identified the unique characters present in the text."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "IalZLbvOzf-F",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "'''TODO: Create a mapping from indices to characters'''\n",
        "idx2char = np.array(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "tZfqhkYCymwX"
      },
      "cell_type": "markdown",
      "source": [
        "This gives us an integer representation for each character. Observe that the unique characters (i.e., our vocabulary) in the text are mapped as indices from 0 to `len(unique)`. Let's take a peek at this numerical representation of our dataset:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "FYyNlCNXymwY",
        "outputId": "c184a8a2-9937-4c66-dfce-a4b5be9f259c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "cell_type": "code",
      "source": [
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  '!' :   2,\n",
            "  ' ' :   1,\n",
            "  '#' :   4,\n",
            "  '\"' :   3,\n",
            "  \"'\" :   5,\n",
            "  ')' :   7,\n",
            "  '(' :   6,\n",
            "  '-' :   9,\n",
            "  ',' :   8,\n",
            "  '/' :  11,\n",
            "  '.' :  10,\n",
            "  '1' :  13,\n",
            "  '0' :  12,\n",
            "  '3' :  15,\n",
            "  '2' :  14,\n",
            "  '5' :  17,\n",
            "  '4' :  16,\n",
            "  '7' :  19,\n",
            "  '6' :  18,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IqxpSuZ1w-ub",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can also look at how the first part of the text is mapped to an integer representation:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "l1VKcQHcymwb",
        "outputId": "862cf0a3-f948-428c-c9c4-ce3abb9c0cd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'X:1\\nT:Alexand' ---- characters mapped to int ---- > [49 22 13  0 45 22 26 67 60 79 56 69 59]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "hgsVvVxnymwf"
      },
      "cell_type": "markdown",
      "source": [
        "### Create training examples and targets\n",
        "\n",
        "Our next step is to actually divide the text into example sequences that we'll use during training. Each input sequence that we feed into our RNN will contain `seq_length` characters from the text. We'll also need to define a target sequence for each input sequence, which will be used in training the RNN to predict the next character. For each input, the corresponding target will contain the same length of text, except shifted one character to the right.\n",
        "\n",
        "To do this, we'll break the text into chunks of `seq_length+1`. Suppose `seq_length` is 4 and our text is \"Hello\". Then, our input sequence is \"Hell\", and the target sequence \"ello\".\n"
      ]
    },
    {
      "metadata": {
        "id": "8jc6cYac0zo1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First, use the [`tf.data.Dataset.from_tensor_slices`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices) function to convert the text vector into a stream of character indices. This is a function within [`tf.data`](https://www.tensorflow.org/api_docs/python/tf/data) which is generally useful for importing data.\n",
        "\n",
        "The [`batch`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch) method will then let us convert this stream of character indices to sequences of the desired size."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "0UHJDA39zf-O",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//seq_length\n",
        "\n",
        "# Create training examples / targets\n",
        "# Note how we are using the `tf.data` module!\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "'''TODO: use the batch function to generate sequences of the desired size'''\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "UbLcIPBj_mWZ"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we need to define the input and target texts for each sequence. \n",
        "\n",
        "Define a function to do this, and then use the [`map`](http://book.pythontips.com/en/latest/map_filter.html) method to apply a simple function to each batch. "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "9NGu-FkO_kYU",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''TODO: define a function that takes a sequence (chunk) and outputs both the input text and target text sequences'''\n",
        "'''Hint: consider the \"Hello\" example'''\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "'''TODO: use the map method to apply your function to the list of sequences to generate the dataset!'''\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "_33OHL3b84i0"
      },
      "cell_type": "markdown",
      "source": [
        "For each of these vectors, each index is processed at a single time step. So, for the input at time step 0, the model receives the index for the first character in the sequence, and tries to predict the index of the next character. At the next timestep, it does the same thing but the `RNN` considers the information from the previous step, i.e., it's updated state, in addition to the current input.\n",
        "\n",
        "We can make this concrete by taking a look at how this works over the first several characters in our text:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "0eBu9WZG84i0",
        "outputId": "fb8151fc-bed3-4c1d-dac7-29eb1f2907a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "  input: 49 ('X')\n",
            "  expected output: 22 (':')\n",
            "Step    1\n",
            "  input: 22 (':')\n",
            "  expected output: 13 ('1')\n",
            "Step    2\n",
            "  input: 13 ('1')\n",
            "  expected output: 0 ('\\n')\n",
            "Step    3\n",
            "  input: 0 ('\\n')\n",
            "  expected output: 45 ('T')\n",
            "Step    4\n",
            "  input: 45 ('T')\n",
            "  expected output: 22 (':')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "MJdfPmdqzf-R"
      },
      "cell_type": "markdown",
      "source": [
        "### Create training batches\n",
        "\n",
        "Great! Now we have our text split into sequences of manageable size. But before we actually feed this data into our model, we'll [`shuffle`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle) the data (for the purpose of stochastic gradient descent) and then pack it into batches which will be used during training."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "p2pGotuNzf-S",
        "outputId": "0efbeb52-5d7f-4123-8d6c-b74372aae06f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Batch size \n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
        "\n",
        "# Buffer size is similar to a queue size\n",
        "# This defines a manageable data size to put into memory, where elements are shuffled\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# Examine the dimensions of the dataset\n",
        "dataset"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "r6oUuElIMgVx"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.4 The Recurrent Neural Network (RNN) Model"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "m8gPwEjRzf-Z"
      },
      "cell_type": "markdown",
      "source": [
        "We will now define and train a RNN model on our ABC music dataset, and then use that trained model to generate a new song. We will train our RNN using batches of song snippets from our dataset.\n",
        "\n",
        "This model will be based off a single LSTM cell, with a state vector used to maintain temporal dependencies between consecutive music notes. At each time step, we feed in a sequence of previous notes. The final output of the LSTM (i.e., of the last unit) is fed in to a single fully connected layer to output a probability distribution over the next note. In this way, we model the probability distribution\n",
        "\n",
        "Use `tf.keras.Sequential` to define the model. For this simple example three layers are used to define our model:\n",
        "\n",
        "* `tf.keras.layers.Embedding`: The input layer. A trainable lookup table that will map the numbers of each character to a vector with `embedding_dim` dimensions;\n",
        "* `tf.keras.layers.LSTM`: A type of RNN with size `units=rnn_units` \n",
        "* `tf.keras.layers.Dense`: The output layer, with `vocab_size` outputs.\n",
        "\n",
        "TODO: make sure this matches the code!"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "zHT8cLh7EAsg",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension \n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6_0TU6FrlXq0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "network learns how to embed sequence in meaningful way. this embedding is then fed into the LSTM\n",
        "outside the LSTM, feed into dense layer which outputs a softmax over the vocab size"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "NNVB-jmMEOzP"
      },
      "cell_type": "markdown",
      "source": [
        "Next define a function to build the model.\n",
        "\n",
        "Use `CuDNNLSTM` if running on GPU.  "
      ]
    },
    {
      "metadata": {
        "id": "OQH6dTWilLfU",
        "colab_type": "code",
        "outputId": "74a6e8ef-56a4-4fcf-b4e2-787489337e5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "tf.test.is_gpu_available()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "DHsTugpJlLD6",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if tf.test.is_gpu_available():\n",
        "  rnn = tf.keras.layers.CuDNNLSTM\n",
        "else:\n",
        "  import functools\n",
        "  rnn = functools.partial(\n",
        "    tf.keras.layers.LSTM, recurrent_activation='sigmoid')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "MtCrdfzEI2N0",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, \n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    rnn(rnn_units,\n",
        "        return_sequences=True, \n",
        "        recurrent_initializer='glorot_uniform',\n",
        "        stateful=True),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "wwsrpOik5zhv",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab), \n",
        "  embedding_dim=embedding_dim, \n",
        "  rnn_units=rnn_units, \n",
        "  batch_size=BATCH_SIZE)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IbWU4dMJmMvq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TODO: create student TODOs within build_model. Note return_sequences, recurrent_initializer, stateful the students may not know about... the main thing for the TODO could be the sizing (Embedding, rnn, Dense)"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "-ubPo0_9Prjb"
      },
      "cell_type": "markdown",
      "source": [
        "## Try the model\n",
        "\n",
        "Now run the model to see that it behaves as expected.\n",
        "\n",
        "First check the shape of the output:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "C-_70kKAPrPU",
        "outputId": "47db5269-1e9b-4c51-83c9-c65f7c884325",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1): \n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(TensorShape([Dimension(64), Dimension(100), Dimension(83)]), '# (batch_size, sequence_length, vocab_size)')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Q6NzLBi4VM4o"
      },
      "cell_type": "markdown",
      "source": [
        "In the above example the sequence length of the input is `100` but the model can be run on inputs of any length: "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "vPGmAAXmVLGC",
        "outputId": "4761f904-64aa-403b-9755-af127fd2c0d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           21248     \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm (CuDNNLSTM)       (64, None, 1024)          5251072   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 83)            85075     \n",
            "=================================================================\n",
            "Total params: 5,357,395\n",
            "Trainable params: 5,357,395\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fAzDX2f-noab",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note about model summary: check the layers, the shape of the output of each of the layers, batch size, etc./\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "uwv0gEkURfx1"
      },
      "cell_type": "markdown",
      "source": [
        "To get actual predictions from the model we need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary. \n",
        "\n",
        "Note: It is important to _sample_ from this distribution as taking the _argmax_ of the distribution can easily get the model stuck in a loop.\n",
        "\n",
        "Try it for the first example in the batch:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "4V4MfFg0RQJg",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.multinomial(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "QM1Vbxs_URw5"
      },
      "cell_type": "markdown",
      "source": [
        "This gives us, at each timestep, a prediction of the next character index:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "YqFMUQc_UFgM",
        "outputId": "91656c3b-63d0-4924-edda-cb427dbbf9d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([82, 76, 59, 40, 47, 39, 26, 78, 41, 31, 14, 67, 62, 59, 69, 79, 69,\n",
              "       79, 29, 34, 32,  0, 30, 79,  7, 37,  9, 29, 33, 56,  0, 49, 53, 21,\n",
              "       44, 19,  8, 13, 59,  4, 79, 73, 82, 31, 51,  2, 68, 23, 45, 18, 79,\n",
              "       50, 47, 49, 38, 36, 28, 30, 23, 43, 28, 80, 53, 20,  3, 34, 74, 39,\n",
              "       19, 22, 70, 30, 51,  5, 76, 19, 57, 33, 18, 78,  6, 72, 40, 10, 82,\n",
              "       51, 55, 65, 74, 15, 78, 30, 42, 74, 36, 78, 54, 40,  1,  5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "LfLtsP3mUhCG"
      },
      "cell_type": "markdown",
      "source": [
        "Decode these to see the text predicted by this untrained model:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "xWcFwPwLSo05",
        "outputId": "8e51745e-463d-4bd0-81e8-b7f8a496e280",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Input: \\n', '\"3|]!\\\\n\\\\nX:100\\\\nT:Father Kelly\\'s No. 1\\\\nZ: id:dc-reel-92\\\\nM:C\\\\nL:1/8\\\\nK:G Major\\\\nGA|B2GB AGEG|DGGF G2AB|cBAB \"')\n",
            "()\n",
            "('Next Char Predictions: \\n', '\\'=\\\\\\':XJnv3tS OX<2uz3G/\\\\nbnjO:(!S1:=)gCb/ldHe9S59Eu!Z\\\\nEt[swF^j#_4vw\"/Ui.UXuFIb6ubFzZs3r<h]!j!hd:_3)64MMF\\'')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "LJL0Q0YPY6Ee"
      },
      "cell_type": "markdown",
      "source": [
        "## Train the model"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "YCbHQHiaa4Ic"
      },
      "cell_type": "markdown",
      "source": [
        "At this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "4HrXTACTdzY-",
        "outputId": "b694a0e5-1e6b-444d-95ae-4590b6dae280",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "def compute_loss(labels, logits):\n",
        "  return tf.keras.backend.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = compute_loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\") \n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Prediction shape: ', TensorShape([Dimension(64), Dimension(100), Dimension(83)]), ' # (batch_size, sequence_length, vocab_size)')\n",
            "('scalar_loss:      ', 4.4173813)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "65U2xVdQoh1r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "use sparse_categorical_crossentropy because it is categorical classification task\n",
        "TODO: potentially incorporate class TODO here?"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "d4tSNwymzf-q",
        "outputId": "c6ad5b96-cd0f-4e3d-8540-03603991c594",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Training step\n",
        "EPOCHS = 5\n",
        "optimizer = tf.train.AdamOptimizer()\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    # initializing the hidden state at the start of every epoch\n",
        "    # initally hidden is None\n",
        "    hidden = model.reset_states()\n",
        "\n",
        "    progress_bar = tqdm(enumerate(dataset))\n",
        "    for (batch_n, (inp, target)) in progress_bar:\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # feeding the hidden state back into the model\n",
        "            # This is the interesting step\n",
        "            predictions = model(inp)\n",
        "            loss = compute_loss(target, predictions)\n",
        "\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        progress_bar.set_description(\"loss {0:.2f}\".format(loss.numpy().mean()))\n",
        "\n",
        "    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[============================================================] 1/1"
            ],
            "text/html": [
              "<span></span><progress style='margin:2px 4px;' max='1' value='1'></progress>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ],
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>loss 3.23</span><progress style='description_width:initial;margin:2px 4px;' max='1' value='1'></progress>30it [00:05,  5.60it/s]</div>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[============================================================] 1/1"
            ],
            "text/html": [
              "<span></span><progress style='margin:2px 4px;' max='1' value='1'></progress>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ],
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>loss 2.44</span><progress style='description_width:initial;margin:2px 4px;' max='1' value='1'></progress>30it [00:05,  5.58it/s]</div>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[============================================================] 1/1"
            ],
            "text/html": [
              "<span></span><progress style='margin:2px 4px;' max='1' value='1'></progress>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ],
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>loss 1.84</span><progress style='description_width:initial;margin:2px 4px;' max='1' value='1'></progress>30it [00:05,  5.56it/s]</div>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[============================================================] 1/1"
            ],
            "text/html": [
              "<span></span><progress style='margin:2px 4px;' max='1' value='1'></progress>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ],
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>loss 1.69</span><progress style='description_width:initial;margin:2px 4px;' max='1' value='1'></progress>30it [00:05,  5.56it/s]</div>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[============================================================] 1/1"
            ],
            "text/html": [
              "<span></span><progress style='margin:2px 4px;' max='1' value='1'></progress>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ],
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>loss 1.56</span><progress style='description_width:initial;margin:2px 4px;' max='1' value='1'></progress>30it [00:05,  5.56it/s]</div>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "kKkD5M6eoSiN"
      },
      "cell_type": "markdown",
      "source": [
        "## Generate text"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "JIPcXllKjkdr"
      },
      "cell_type": "markdown",
      "source": [
        "Now want to do inference -- use batch size 1 to keep simple\n",
        "\n",
        "### Restore the latest checkpoint"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "LyeYRiuVjodY"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "To keep this prediction step simple, use a batch size of 1.\n",
        "\n",
        "Because of the way the RNN state is passed from timestep to timestep, the model only accepts a fixed batch size once built. \n",
        "\n",
        "To run the model with a different `batch_size`, we need to rebuild the model and restore the weights from the checkpoint.\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "LycQ-ot_jjyu",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "71xa6jnYVrAN",
        "outputId": "edbc1934-7772-4c99-8eaf-19ef55af3e2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            21248     \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_1 (CuDNNLSTM)     (1, None, 1024)           5251072   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 83)             85075     \n",
            "=================================================================\n",
            "Total params: 5,357,395\n",
            "Trainable params: 5,357,395\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "DjGz1tDkzf-u"
      },
      "cell_type": "markdown",
      "source": [
        "### The prediction loop\n",
        "\n",
        "The following code block generates the text:\n",
        "\n",
        "* It Starts by choosing a start string, initializing the RNN state and setting the number of characters to generate.\n",
        "\n",
        "* Get the prediction distribution of the next character using the start string and the RNN state.\n",
        "\n",
        "* Then, use a multinomial distribution to calculate the index of the predicted character. Use this predicted character as our next input to the model.\n",
        "\n",
        "* The RNN state returned by the model is fed back into the model so that it now has more context, instead than only one word. After predicting the next word, the modified RNN states are again fed back into the model, which is how it learns as it gets more context from the previously predicted words.\n",
        "\n",
        "\n",
        "![To generate text the model's output is fed back to the input](https://tensorflow.org/tutorials/sequences/images/text_generation_sampling.png)\n",
        "\n",
        "Looking at the generated text, you'll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "WvuwZBX5Ogfd",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string, generation_length=1000):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing) \n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in tqdm(range(generation_length)):\n",
        "      predictions = model(input_eval)\n",
        "      \n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a multinomial distribution to predict the word returned by the model\n",
        "      predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
        "      \n",
        "      # We pass the predicted word as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "      \n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ktovv0RFhrkn",
        "outputId": "b26eebb6-3172-4a90-fcd0-392c656dda91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868
        }
      },
      "cell_type": "code",
      "source": [
        "# Experiment by changing the start string\n",
        "print(generate_text(model, start_string=\"X\"))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ],
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span></span><progress style='margin:2px 4px;' max='1000' value='1000'></progress>100% 1000/1000 [00:08&lt;00:00, 118.85it/s]</div>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "X8\">L:llissdy id:hss\n",
            "Z: L:dcdc-reel-f455\n",
            "M:C\n",
            ":1/8\n",
            "K:G Major\n",
            "K:G/G AcB Ac|cAd cBA|A3 D2:|!\n",
            "\n",
            "X:372oTenos Novn\n",
            "Z:\n",
            "d:dce|afag g2ed:|!\n",
            "e|gefg bgag|agbe fge2|dBBG BGAG|dBAG DBGB|AEFG ogf|d2Bg edg2|!\n",
            "cdBd efga|efef e2f|BFB f2gufag gfa|fagb ed:|!\n",
            "faa be2|fedg bef2|g2fd efge|gegf ggg:|!\n",
            "\n",
            "X:18\n",
            "T:Boste Shhesor\n",
            "Z:16\n",
            "L:1/8\n",
            "K:G Mojor\n",
            "GF2|c2fd efd|FAD DEF|EDA DFG ecBA|EDD G2:|!\n",
            "GEG FADG|G3AG GGA2|ADFG EFA2:|!\n",
            "fefg e2de|AcAF|GEGF E2AG|FABA d2d:|!\n",
            "A2de a2ge|ced2 edfd|gfd^c d2:|!\n",
            "A\n",
            "BA|FAc2 (3aA|ced cA AF=|dBA cEA|EFE D2:|!\n",
            "\n",
            "X:148\n",
            "T:Gasopeoros\n",
            "Z: i:dc--1iear\n",
            "e,2\n",
            "M:6/8\n",
            "T: Maony\n",
            " F|F'D2 DFG|AFA ec|efd G2E|!\n",
            "AGB c^dBc|e2a^ Aca^g|(faeg dgef|d4AB dBcA|!\n",
            "d^cdd cdBA|(AedB defe|ffde fdAB|!\n",
            "AAce^c2ccB ggfe|b2d2 gfd (eAFG E2dB|!\n",
            "f3^d f2ge|edegB d2cB|GDcc d2B|!\n",
            "gfaf agfg|gaaf agde|!\n",
            "AGF2 G2B|efdc f2EB|cAdB cAFD|A2dBd geg2|!\n",
            "B2cA ^ggg|f3g geed FCZ:1s/8\n",
            "L:1/8\n",
            "K:D Major\n",
            "Z:2 itddc-\n",
            "M:1/8\n",
            "L:1/8\n",
            "K:EMaMan\n",
            "an cafe|E2Ac BcBd|fBGA FdcA|edgf (fdeg aga|fgf fef|!\n",
            "d2ge g2ed|afaf gedf|afg^dB AABc|!\n",
            "ege agf|de^a dfe|gaf efe|fee ed|\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "AM2Uma_-yVIq"
      },
      "cell_type": "markdown",
      "source": [
        "The easiest thing you can do to improve the results it to train it for longer (try `EPOCHS=30`).\n",
        "\n",
        "You can also experiment with a different start string, or try adding another RNN layer to improve the model's accuracy, or adjusting the temperature parameter to generate more or less random predictions."
      ]
    },
    {
      "metadata": {
        "id": "Nryx9psRqkYw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}