{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab2_Part2-Solutions.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aamini/introtodeeplearning_labs/blob/2019/Lab2_Part2_Solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Ag_e7xtTzT1W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Laboratory 2 : Computer Vision\n",
        "\n",
        "# Part 2: De-Biasing Facial Recognition Systems\n",
        "\n",
        "In the second portion of this lab, we will explore two prominent aspects of applied deep learning: facial recognition systems and algorithmic bias. \n",
        "\n",
        "Deploying fair, unbiased AI systems is critical for long-term acceptance of these approaches. Consider the task of facial recognition: given an image, is it an image of a face? [Recent work from the MIT Media Lab](http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf) showed that this seemingly simple, but extremely important, task is subject to extreme amounts of algorithmic bias among select demographics. [Another report](https://ieeexplore.ieee.org/document/6327355) analyzed the face detection system used by the US law enforcement, and found that it had significantly lower accuracy among dark women between the age of 18-30 years old. These results are especially concerning since these facial recognition systems are almost never deployed in isolation.\n",
        "\n",
        "We'll investigate one approach to addressing this problem by building a facial recognition model that learns the underlying *latent variables* in a dataset and uses this to adaptively re-sample the training data, mitigating any bias. This lab is based on a very recent paper in which this approach was originially proposed.   \n",
        "\n",
        "Let's get started.\n",
        "\n",
        "TODO: cite Face paper"
      ]
    },
    {
      "metadata": {
        "id": "3Ezfc6Yv6IhI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First let's install the relevant dependencies:"
      ]
    },
    {
      "metadata": {
        "id": "E46sWVKK6LP9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO: install dependencies"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V0e77oOM3udR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.1 Datasets\n",
        "\n",
        "We'll be using three datasets in this lab. In order to train our facial recognition models, we'll need a dataset of positive examples (i.e., of faces) and a dataset of negative examples (i.e., of things that are not faces). Finally, we'll need a test dataset of face images. Since we're concerned about the potential *bias* of our learned models against certain demographics, it's important that the test dataset we use has equal representation across the demographics or features of interest. We'll specifically be looking at skin tone and gender. \n",
        "\n",
        "\n",
        "1.   Positive training data: [CelebA Dataset](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html). A large-scale (over 200K images) of celebrity faces.   \n",
        "2.   Negative training data: [ImageNet](http://www.image-net.org/). Many images across many different categories. We'll take negative examples from a variety of non-human categories. \n",
        "3. Test data: [Pilot Parliaments Benchmark](http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf) (PPB). Images of parliamentarians from three African countries and three European countries, selected for parity across gender and skin tone. \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "CQ146crd6RdA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's begin by importing these three datasets. We've written a function that does a bit of data pre-processing and imports these data for you :)"
      ]
    },
    {
      "metadata": {
        "id": "RWXaaIWy6jVw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO function to import the datasets in the appropriate format"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sxtkJoqF6oH1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To get a better sense of what's in each of these datasets, we can display some randomly selected images from each."
      ]
    },
    {
      "metadata": {
        "id": "4B4egQZY6wEt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO display images from the three datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JpgJdNyJ60dy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As you can see, the PPB dataset is quite balanced in terms of the skin tone and gender across the set of displayed images. Do you notice any trends or patterns in the images from the CelebA dataset? Do you anticipate any potential issues in terms of classification performance for models trained on CelebA and then tested on a dataset like PPB?"
      ]
    },
    {
      "metadata": {
        "id": "NDj7KBaW8Asz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Thinking about bias\n",
        "\n",
        "Remember that we'll be training facial detection classifiers on the large, well-curated CelebA dataset (and ImageNet), and then evaluate the *accuracy* and *bias* of our models across different demographics by testing them on the PPB dataset. Our goal is to build a model that trains on CelebA *and* achieves high classification accuracy on PPB across all demographics, and to show that this model does not suffer from algorithmic bias. \n",
        "\n",
        "What exactly do we mean if we say a classifier is biased? In order to formalize this, we'll need to think about [*latent variables*](https://en.wikipedia.org/wiki/Latent_variable), variables that define a dataset but are not strictly observed, which was introduced during the generative modeling lecture. We can think of a classifier as *biased* if its classification decision changes after it sees some additional latent features. This notion of bias will be helpful to keep in mind throughout the rest of the lab. "
      ]
    },
    {
      "metadata": {
        "id": "AIFDvU4w8OIH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.2 CNN for facial detection \n",
        "\n",
        "First, we'll define and train a CNN on the facial classification task, and evaluate its accuracy on the PPB dataset. Later, we'll evaluate the performance of our debiased models against this baseline CNN. The model architecture is shown here:\n",
        "\n",
        "![CNN model](img/mnist_model.png \"CNN Architecture for MNIST Classification\")\n",
        "TODO: update the architecture figure"
      ]
    },
    {
      "metadata": {
        "id": "82EVTAAW7B_X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}